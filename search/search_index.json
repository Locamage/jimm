{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"jimm docs","text":"<p>These are the docs for the Jax Image Modeling of Models library. These models are implemented using flax nnx/jax with sharding annotations for each model.</p>"},{"location":"#models-implemented","title":"Models Implemented","text":"<ul> <li>Vision Transformers</li> <li>CLIP</li> <li>more tbd</li> <li>contribute, it's open source!</li> </ul>"},{"location":"models/CLIP/","title":"CLIP (Contrastive Language\u2013Image Pre-training)","text":"<p>CLIP (Contrastive Language\u2013Image Pre-training) is a neural network architecture that learns visual concepts from natural language supervision. It is trained on a large dataset of image-text pairs to create a unified vision-language model that can understand both images and text in a shared semantic space.</p> <p>CLIP consists of two main components: 1. A vision encoder (Vision Transformer) that processes images into visual features 2. A text encoder (Transformer) that processes text into textual features</p> <p>The model is trained using contrastive learning, where it learns to maximize the cosine similarity between the embeddings of matching image-text pairs while minimizing it for non-matching pairs. This allows CLIP to perform zero-shot classification by comparing image embeddings with text embeddings of potential labels.</p> <p>CLIP was introduced in the paper \"Learning Transferable Visual Models From Natural Language Supervision\" and has shown remarkable zero-shot generalization capabilities across a wide range of visual classification tasks.</p> <p>The CLIP Vision Transformer (ViT) is a variant of the standard Vision Transformer architecture, adapted specifically for the CLIP model. It processes images into visual features that can be matched with text embeddings.</p> <p>The CLIP model combines a Vision Transformer and a Text Transformer to learn joint representations of images and text. It is trained to maximize the similarity between matching image-text pairs while minimizing similarity between non-matching pairs.</p>"},{"location":"models/CLIP/#jimm.models.clip.CLIPVisionTransformer","title":"<code>jimm.models.clip.CLIPVisionTransformer</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/jimm/models/clip.py</code> <pre><code>class CLIPVisionTransformer(nnx.Module):\n    def __init__(\n        self,\n        input_resolution: int,\n        patch_size: int,\n        width: int,\n        layers: int,\n        num_heads: int,\n        output_dim: int,\n        rngs: nnx.Rngs = nnx.Rngs(0),\n        dtype: DTypeLike = jnp.float32,\n        param_dtype: DTypeLike = jnp.float32,\n        mesh: Mesh | None = None,\n    ):\n        \"\"\"\n        Initialize the Vision Transformer.\n\n        Args:\n            input_resolution (int): The resolution of the input images.\n            patch_size (int): The patch size of the vision transformer.\n            width (int): The width of the vision transformer.\n            layers (int): The number of layers in the vision transformer.\n            num_heads (int): The number of attention heads in the vision transformer.\n            output_dim (int): The output dimension of the vision transformer.\n            rngs (nnx.Rngs): The random number generator state. Defaults to nnx.Rngs(0).\n            dtype (DTypeLike): The data type for computations. Defaults to jnp.float32.\n            param_dtype (DTypeLike): The data type for parameters. Defaults to jnp.float32.\n            mesh (Mesh | None): The device mesh for parameter sharding.\n        \"\"\"\n        n_patches: int = (input_resolution // patch_size) ** 2\n        self.input_resolution = input_resolution\n        self.output_dim = output_dim\n        self.conv1 = nnx.Conv(\n            in_features=3,\n            out_features=width,\n            kernel_size=(patch_size, patch_size),\n            strides=(patch_size, patch_size),\n            padding=\"VALID\",\n            use_bias=False,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            rngs=rngs,\n            kernel_init=sharded_init(nnx.initializers.xavier_uniform(), P(None, None, None, \"model\"), mesh),\n            bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n        )\n        _cls_token_initializer = sharded_init(nnx.initializers.zeros_init(), P(None, None, \"model\"), mesh)\n        cls_token_value: Float[Array, \"1 1 width\"] = _cls_token_initializer(rngs.params(), (1, 1, width))\n        self.cls_token = nnx.Param(cls_token_value)\n        _position_embeddings_initializer = sharded_init(nnx.initializers.truncated_normal(stddev=0.02), P(None, None, \"model\"), mesh)\n        pos_emb_value: Float[Array, \"1 n_patches+1 width\"] = _position_embeddings_initializer(rngs.params(), (1, n_patches + 1, width))\n        self.position_embeddings = nnx.Param(pos_emb_value)\n\n        self.ln_pre = nnx.LayerNorm(\n            width,\n            epsilon=1e-5,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            rngs=rngs,\n            scale_init=sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh),\n            bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n        )\n        self.transformer = Transformer(\n            width=width,\n            mlp_dim=width * 4,\n            layers=layers,\n            num_heads=num_heads,\n            dropout_rate=0.0,\n            use_quick_gelu=True,\n            rngs=rngs,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            mesh=mesh,\n        )\n        self.ln_post = nnx.LayerNorm(\n            width,\n            epsilon=1e-5,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            rngs=rngs,\n            scale_init=sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh),\n            bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n        )\n\n        self.proj = nnx.Linear(\n            width,\n            output_dim,\n            use_bias=False,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            rngs=rngs,\n            kernel_init=sharded_init(nnx.initializers.xavier_uniform(), P(None, \"model\"), mesh),\n        )\n\n    def __call__(self, x: Float[Array, \"batch height width channels\"]) -&gt; Float[Array, \"batch output_dim\"]:\n        \"\"\"\n        Apply the CLIP vision transformer to input images.\n\n        Args:\n            x: Float[Array, \"batch height width channels\"]\n                Batch of input images with shape (batch, height, width, channels).\n\n        Returns:\n            Float[Array, \"batch output_dim\"]\n                Batch of output embeddings with shape (batch, output_dim).\n        \"\"\"\n        patches: Float[Array, \"batch patches_h patches_w width\"] = self.conv1(x)\n        batch_size = patches.shape[0]\n        patches: Float[Array, \"batch n_patches width\"] = patches.reshape(batch_size, -1, patches.shape[-1])\n        cls_token: Float[Array, \"batch 1 width\"] = jnp.tile(self.cls_token.value, [batch_size, 1, 1])\n        x: Float[Array, \"batch n_patches+1 width\"] = jnp.concat([cls_token, patches], axis=1)\n        embeddings: Float[Array, \"batch n_patches+1 width\"] = x + self.position_embeddings.value\n        x: Float[Array, \"batch n_patches+1 width\"] = self.ln_pre(embeddings)\n        x: Float[Array, \"batch n_patches+1 width\"] = self.transformer(x)\n        x: Float[Array, \"batch n_patches+1 width\"] = self.ln_post(x)\n        x: Float[Array, \"batch width\"] = x[:, 0]\n        x: Float[Array, \"batch output_dim\"] = self.proj(x)\n        return x\n</code></pre>"},{"location":"models/CLIP/#jimm.models.clip.CLIPVisionTransformer.__call__","title":"<code>__call__(x)</code>","text":"<p>Apply the CLIP vision transformer to input images.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, 'batch height width channels']</code> <p>Float[Array, \"batch height width channels\"] Batch of input images with shape (batch, height, width, channels).</p> required <p>Returns:</p> Type Description <code>Float[Array, 'batch output_dim']</code> <p>Float[Array, \"batch output_dim\"] Batch of output embeddings with shape (batch, output_dim).</p> Source code in <code>src/jimm/models/clip.py</code> <pre><code>def __call__(self, x: Float[Array, \"batch height width channels\"]) -&gt; Float[Array, \"batch output_dim\"]:\n    \"\"\"\n    Apply the CLIP vision transformer to input images.\n\n    Args:\n        x: Float[Array, \"batch height width channels\"]\n            Batch of input images with shape (batch, height, width, channels).\n\n    Returns:\n        Float[Array, \"batch output_dim\"]\n            Batch of output embeddings with shape (batch, output_dim).\n    \"\"\"\n    patches: Float[Array, \"batch patches_h patches_w width\"] = self.conv1(x)\n    batch_size = patches.shape[0]\n    patches: Float[Array, \"batch n_patches width\"] = patches.reshape(batch_size, -1, patches.shape[-1])\n    cls_token: Float[Array, \"batch 1 width\"] = jnp.tile(self.cls_token.value, [batch_size, 1, 1])\n    x: Float[Array, \"batch n_patches+1 width\"] = jnp.concat([cls_token, patches], axis=1)\n    embeddings: Float[Array, \"batch n_patches+1 width\"] = x + self.position_embeddings.value\n    x: Float[Array, \"batch n_patches+1 width\"] = self.ln_pre(embeddings)\n    x: Float[Array, \"batch n_patches+1 width\"] = self.transformer(x)\n    x: Float[Array, \"batch n_patches+1 width\"] = self.ln_post(x)\n    x: Float[Array, \"batch width\"] = x[:, 0]\n    x: Float[Array, \"batch output_dim\"] = self.proj(x)\n    return x\n</code></pre>"},{"location":"models/CLIP/#jimm.models.clip.CLIPVisionTransformer.__init__","title":"<code>__init__(input_resolution, patch_size, width, layers, num_heads, output_dim, rngs=nnx.Rngs(0), dtype=jnp.float32, param_dtype=jnp.float32, mesh=None)</code>","text":"<p>Initialize the Vision Transformer.</p> <p>Parameters:</p> Name Type Description Default <code>input_resolution</code> <code>int</code> <p>The resolution of the input images.</p> required <code>patch_size</code> <code>int</code> <p>The patch size of the vision transformer.</p> required <code>width</code> <code>int</code> <p>The width of the vision transformer.</p> required <code>layers</code> <code>int</code> <p>The number of layers in the vision transformer.</p> required <code>num_heads</code> <code>int</code> <p>The number of attention heads in the vision transformer.</p> required <code>output_dim</code> <code>int</code> <p>The output dimension of the vision transformer.</p> required <code>rngs</code> <code>Rngs</code> <p>The random number generator state. Defaults to nnx.Rngs(0).</p> <code>Rngs(0)</code> <code>dtype</code> <code>DTypeLike</code> <p>The data type for computations. Defaults to jnp.float32.</p> <code>float32</code> <code>param_dtype</code> <code>DTypeLike</code> <p>The data type for parameters. Defaults to jnp.float32.</p> <code>float32</code> <code>mesh</code> <code>Mesh | None</code> <p>The device mesh for parameter sharding.</p> <code>None</code> Source code in <code>src/jimm/models/clip.py</code> <pre><code>def __init__(\n    self,\n    input_resolution: int,\n    patch_size: int,\n    width: int,\n    layers: int,\n    num_heads: int,\n    output_dim: int,\n    rngs: nnx.Rngs = nnx.Rngs(0),\n    dtype: DTypeLike = jnp.float32,\n    param_dtype: DTypeLike = jnp.float32,\n    mesh: Mesh | None = None,\n):\n    \"\"\"\n    Initialize the Vision Transformer.\n\n    Args:\n        input_resolution (int): The resolution of the input images.\n        patch_size (int): The patch size of the vision transformer.\n        width (int): The width of the vision transformer.\n        layers (int): The number of layers in the vision transformer.\n        num_heads (int): The number of attention heads in the vision transformer.\n        output_dim (int): The output dimension of the vision transformer.\n        rngs (nnx.Rngs): The random number generator state. Defaults to nnx.Rngs(0).\n        dtype (DTypeLike): The data type for computations. Defaults to jnp.float32.\n        param_dtype (DTypeLike): The data type for parameters. Defaults to jnp.float32.\n        mesh (Mesh | None): The device mesh for parameter sharding.\n    \"\"\"\n    n_patches: int = (input_resolution // patch_size) ** 2\n    self.input_resolution = input_resolution\n    self.output_dim = output_dim\n    self.conv1 = nnx.Conv(\n        in_features=3,\n        out_features=width,\n        kernel_size=(patch_size, patch_size),\n        strides=(patch_size, patch_size),\n        padding=\"VALID\",\n        use_bias=False,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        rngs=rngs,\n        kernel_init=sharded_init(nnx.initializers.xavier_uniform(), P(None, None, None, \"model\"), mesh),\n        bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n    )\n    _cls_token_initializer = sharded_init(nnx.initializers.zeros_init(), P(None, None, \"model\"), mesh)\n    cls_token_value: Float[Array, \"1 1 width\"] = _cls_token_initializer(rngs.params(), (1, 1, width))\n    self.cls_token = nnx.Param(cls_token_value)\n    _position_embeddings_initializer = sharded_init(nnx.initializers.truncated_normal(stddev=0.02), P(None, None, \"model\"), mesh)\n    pos_emb_value: Float[Array, \"1 n_patches+1 width\"] = _position_embeddings_initializer(rngs.params(), (1, n_patches + 1, width))\n    self.position_embeddings = nnx.Param(pos_emb_value)\n\n    self.ln_pre = nnx.LayerNorm(\n        width,\n        epsilon=1e-5,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        rngs=rngs,\n        scale_init=sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh),\n        bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n    )\n    self.transformer = Transformer(\n        width=width,\n        mlp_dim=width * 4,\n        layers=layers,\n        num_heads=num_heads,\n        dropout_rate=0.0,\n        use_quick_gelu=True,\n        rngs=rngs,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        mesh=mesh,\n    )\n    self.ln_post = nnx.LayerNorm(\n        width,\n        epsilon=1e-5,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        rngs=rngs,\n        scale_init=sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh),\n        bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n    )\n\n    self.proj = nnx.Linear(\n        width,\n        output_dim,\n        use_bias=False,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        rngs=rngs,\n        kernel_init=sharded_init(nnx.initializers.xavier_uniform(), P(None, \"model\"), mesh),\n    )\n</code></pre>"},{"location":"models/CLIP/#jimm.models.clip.CLIP","title":"<code>jimm.models.clip.CLIP</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/jimm/models/clip.py</code> <pre><code>class CLIP(nnx.Module):\n    def __init__(\n        self,\n        image_resolution: int,\n        vision_layers: int,\n        vision_width: int,\n        vision_patch_size: int,\n        context_length: int,\n        vocab_size: int,\n        transformer_width: int,\n        transformer_heads: int,\n        transformer_layers: int,\n        rngs: nnx.Rngs = nnx.Rngs(0),\n        dtype: DTypeLike = jnp.float32,\n        param_dtype: DTypeLike = jnp.float32,\n        mesh: Mesh | None = None,\n    ):\n        \"\"\"\n        Initialize the CLIP model.\n\n        Args:\n            image_resolution (int): The resolution of the input images.\n            vision_layers (int): The number of layers in the vision transformer.\n            vision_width (int): The width of the vision transformer.\n            vision_patch_size (int): The patch size of the vision transformer.\n            context_length (int): The length of the context.\n            vocab_size (int): The size of the vocabulary.\n            transformer_width (int): The width of the transformer.\n            transformer_heads (int): The number of attention heads in the transformer.\n            transformer_layers (int): The number of layers in the transformer.\n            rngs (nnx.Rngs): The random number generator state. Defaults to nnx.Rngs(0).\n            dtype (DTypeLike): The data type for computations. Defaults to jnp.float32.\n            param_dtype (DTypeLike): The data type for parameters. Defaults to jnp.float32.\n            mesh (Mesh | None): The device mesh for parameter sharding. Defaults to None.\n        \"\"\"\n        self.vision_layers = vision_layers\n        self.vision_width = vision_width\n        self.vision_patch_size = vision_patch_size\n        self.context_length = context_length\n        self.vocab_size = vocab_size\n        self.transformer_width = transformer_width\n        self.transformer_heads = transformer_heads\n        self.transformer_layers = transformer_layers\n        self.dtype = dtype\n\n        vision_heads = vision_width // 64\n\n        self.attn_mask: Float[Array, \"context_length context_length\"] = jnp.tril(jnp.ones((context_length, context_length), dtype=dtype))\n\n        self.vision_model = CLIPVisionTransformer(\n            input_resolution=image_resolution,\n            patch_size=vision_patch_size,\n            width=vision_width,\n            layers=vision_layers,\n            num_heads=vision_heads,\n            output_dim=transformer_width,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            mesh=mesh,\n            rngs=rngs,\n        )\n\n        self.text_model = Transformer(\n            width=transformer_width,\n            mlp_dim=transformer_width * 4,\n            layers=transformer_layers,\n            num_heads=transformer_heads,\n            dropout_rate=0.0,\n            attn_mask=self.attn_mask,\n            use_quick_gelu=True,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            mesh=mesh,\n            rngs=rngs,\n        )\n        self.vocab_size = vocab_size\n        self.token_embedding = nnx.Embed(\n            num_embeddings=vocab_size,\n            features=transformer_width,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            rngs=rngs,\n            embedding_init=sharded_init(nnx.initializers.xavier_uniform(), P(\"model\", None), mesh),\n        )\n        self.positional_embedding = nnx.Param(sharded_init(nnx.initializers.truncated_normal(stddev=0.02), P(\"model\", None), mesh)(rngs.params(), (context_length, transformer_width)))\n        self.ln_final = nnx.LayerNorm(\n            transformer_width,\n            epsilon=1e-5,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            rngs=rngs,\n            scale_init=sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh),\n            bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n        )\n        self.text_projection = nnx.Linear(\n            transformer_width,\n            transformer_width,\n            use_bias=False,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            rngs=rngs,\n            kernel_init=sharded_init(nnx.initializers.xavier_uniform(), P(\"model\", None), mesh),\n        )\n        self.logit_scale = nnx.Param(sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh)(rngs.params(), ()))\n\n    def encode_image(self, image: Float[Array, \"batch height width channels\"]) -&gt; Float[Array, \"batch transformer_width\"]:\n        \"\"\"\n        Encode images into embeddings.\n\n        Args:\n            image (Float[Array, \"batch height width channels\"]): Batch of input images.\n\n        Returns:\n            Float[Array, \"batch transformer_width\"]: Image embeddings.\n        \"\"\"\n        return self.vision_model(image)\n\n    def encode_text(self, text: Int[Array, \"batch context_length\"]) -&gt; Float[Array, \"batch transformer_width\"]:\n        \"\"\"\n        Encode text tokens into embeddings.\n\n        Args:\n            text (Int[Array, \"batch context_length\"]): Batch of token sequences.\n\n        Returns:\n            Float[Array, \"batch transformer_width\"]: Text embeddings.\n        \"\"\"\n        seq_len = text.shape[1]\n        x: Float[Array, \"batch context_length transformer_width\"] = self.token_embedding(text)\n        x: Float[Array, \"batch context_length transformer_width\"] = x + self.positional_embedding.value[:seq_len]\n        x: Float[Array, \"batch context_length transformer_width\"] = self.text_model(x)\n        x: Float[Array, \"batch context_length transformer_width\"] = self.ln_final(x)\n\n        eot_token_pos: Float[Array, \" batch \"] = jnp.argmax(text, axis=-1)\n        batch_indices: Float[Array, \" batch \"] = jnp.arange(x.shape[0])\n        x: Float[Array, \"batch transformer_width\"] = x[batch_indices, eot_token_pos] @ self.text_projection.kernel.value\n        return x\n\n    def __call__(self, image: Float[Array, \"batch height width channels\"], text: Int[Array, \"batch context_length\"]) -&gt; Float[Array, \"batch batch\"]:\n        \"\"\"\n        Calculate similarity between image and text embeddings.\n\n        Args:\n            image (Float[Array, \"batch height width channels\"]): Batch of input images.\n            text (Int[Array, \"batch context_length\"]): Batch of token sequences.\n\n        Returns:\n            Float[Array, \"batch batch\"]: Similarity scores between all pairs of images and texts.\n        \"\"\"\n        image_features: Float[Array, \"batch transformer_width\"] = self.encode_image(image)\n        text_features: Float[Array, \"batch transformer_width\"] = self.encode_text(text)\n\n        image_features: Float[Array, \"batch transformer_width\"] = image_features / jnp.linalg.norm(image_features, axis=-1, keepdims=True)\n        text_features: Float[Array, \"batch transformer_width\"] = text_features / jnp.linalg.norm(text_features, axis=-1, keepdims=True)\n\n        logit_scale: Float[Array, \"\"] = jnp.exp(self.logit_scale.value)\n        logits: Float[Array, \"batch batch\"] = logit_scale * image_features @ text_features.T\n        return logits\n\n    @classmethod\n    def from_pretrained(cls, model_name_or_path: str, use_pytorch: bool = False, mesh: Mesh | None = None, dtype: DTypeLike = jnp.float32) -&gt; \"CLIP\":\n        \"\"\"Load a pretrained CLIP model from a local path or HuggingFace Hub.\n\n        Args:\n            model_name_or_path (str): Path to local weights or HuggingFace model ID.\n            use_pytorch (bool): Whether to load from PyTorch weights. Defaults to False.\n            mesh (Mesh|None): Optional device mesh for parameter sharding. Defaults to None.\n            dtype (DTypeLike): Data type for computations. Defaults to jnp.float32.\n\n        Returns:\n            CLIP: Pretrained CLIP model\n        \"\"\"\n        import jax\n\n        params_fstate, config_dict = load_params_and_config(model_name_or_path, use_pytorch)\n\n        config: dict[str, Any] | None = config_dict\n\n        if config is None:\n            if not use_pytorch:\n                text_hidden_size = params_fstate[\"text_model.embeddings.token_embedding.weight\"].shape[1]\n                text_max_pos_embed = params_fstate[\"text_model.embeddings.position_embedding.weight\"].shape[0]\n                text_vocab_size = params_fstate[\"text_model.embeddings.token_embedding.weight\"].shape[0]\n\n                text_num_layers = 0\n                for k_param in params_fstate:\n                    if k_param.startswith(\"text_model.encoder.layers.\") and k_param.endswith(\".self_attn.q_proj.weight\"):\n                        layer_idx = int(k_param.split(\".\")[3])\n                        text_num_layers = max(text_num_layers, layer_idx + 1)\n\n                vision_hidden_size = params_fstate[\"vision_model.embeddings.class_embedding\"].shape[0]\n                vision_patch_size = params_fstate[\"vision_model.embeddings.patch_embedding.weight\"].shape[2]\n                vision_image_size = int((params_fstate[\"vision_model.embeddings.position_embedding.weight\"].shape[0] - 1) ** 0.5) * vision_patch_size\n\n                vision_num_layers = 0\n                for k_param in params_fstate:\n                    if k_param.startswith(\"vision_model.encoder.layers.\") and k_param.endswith(\".self_attn.q_proj.weight\"):\n                        layer_idx = int(k_param.split(\".\")[3])\n                        vision_num_layers = max(vision_num_layers, layer_idx + 1)\n\n                config = {\n                    \"text_config\": {\n                        \"hidden_size\": text_hidden_size,\n                        \"num_attention_heads\": text_hidden_size // 64,\n                        \"num_hidden_layers\": text_num_layers,\n                        \"max_position_embeddings\": text_max_pos_embed,\n                        \"vocab_size\": text_vocab_size,\n                    },\n                    \"vision_config\": {\n                        \"hidden_size\": vision_hidden_size,\n                        \"num_attention_heads\": vision_hidden_size // 64,\n                        \"num_hidden_layers\": vision_num_layers,\n                        \"image_size\": vision_image_size,\n                        \"patch_size\": vision_patch_size,\n                    },\n                }\n            else:\n                raise ValueError(f\"Configuration could not be loaded for PyTorch model {model_name_or_path}\")\n\n        text_config = config[\"text_config\"]\n        vision_config = config[\"vision_config\"]\n\n        model = cls(\n            image_resolution=vision_config[\"image_size\"],\n            vision_layers=vision_config[\"num_hidden_layers\"],\n            vision_width=vision_config[\"hidden_size\"],\n            vision_patch_size=vision_config[\"patch_size\"],\n            context_length=text_config[\"max_position_embeddings\"],\n            vocab_size=text_config[\"vocab_size\"],\n            transformer_width=text_config[\"hidden_size\"],\n            transformer_heads=text_config[\"num_attention_heads\"],\n            transformer_layers=text_config[\"num_hidden_layers\"],\n            mesh=mesh,\n            dtype=dtype,\n            param_dtype=dtype,\n        )\n\n        flax_model_params_fstate = dict(nnx.to_flat_state(nnx.state(model, nnx.Param)))\n\n        mapping_list = [\n            ((\"logit_scale\",), (\"logit_scale\",)),\n            ((\"positional_embedding\",), (\"text_model\", \"embeddings\", \"position_embedding\", \"weight\")),\n            ((\"token_embedding\", \"embedding\"), (\"text_model\", \"embeddings\", \"token_embedding\", \"weight\")),\n            ((\"ln_final\", \"scale\"), (\"text_model\", \"final_layer_norm\", \"weight\")),\n            ((\"ln_final\", \"bias\"), (\"text_model\", \"final_layer_norm\", \"bias\")),\n            ((\"text_projection\", \"kernel\"), (\"text_projection\", \"weight\")),\n            ((\"vision_model\", \"cls_token\"), (\"vision_model\", \"embeddings\", \"class_embedding\")),\n            ((\"vision_model\", \"position_embeddings\"), (\"vision_model\", \"embeddings\", \"position_embedding\", \"weight\")),\n            ((\"vision_model\", \"conv1\", \"kernel\"), (\"vision_model\", \"embeddings\", \"patch_embedding\", \"weight\")),\n            ((\"vision_model\", \"ln_pre\", \"scale\"), (\"vision_model\", \"pre_layrnorm\", \"weight\")),\n            ((\"vision_model\", \"ln_pre\", \"bias\"), (\"vision_model\", \"pre_layrnorm\", \"bias\")),\n            ((\"vision_model\", \"ln_post\", \"scale\"), (\"vision_model\", \"post_layernorm\", \"weight\")),\n            ((\"vision_model\", \"ln_post\", \"bias\"), (\"vision_model\", \"post_layernorm\", \"bias\")),\n            ((\"vision_model\", \"proj\", \"kernel\"), (\"visual_projection\", \"weight\")),\n        ]\n\n        for i in range(text_config[\"num_hidden_layers\"]):\n            flax_base = (\"text_model\", \"blocks\", \"layers\", i)\n            hf_base = (\"text_model\", \"encoder\", \"layers\", str(i))\n\n            mapping_list.extend(\n                [\n                    (flax_base + (\"attn\", \"query\", \"kernel\"), hf_base + (\"self_attn\", \"q_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"query\", \"bias\"), hf_base + (\"self_attn\", \"q_proj\", \"bias\")),\n                    (flax_base + (\"attn\", \"key\", \"kernel\"), hf_base + (\"self_attn\", \"k_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"key\", \"bias\"), hf_base + (\"self_attn\", \"k_proj\", \"bias\")),\n                    (flax_base + (\"attn\", \"value\", \"kernel\"), hf_base + (\"self_attn\", \"v_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"value\", \"bias\"), hf_base + (\"self_attn\", \"v_proj\", \"bias\")),\n                    (flax_base + (\"attn\", \"out\", \"kernel\"), hf_base + (\"self_attn\", \"out_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"out\", \"bias\"), hf_base + (\"self_attn\", \"out_proj\", \"bias\")),\n                    (flax_base + (\"norm1\", \"scale\"), hf_base + (\"layer_norm1\", \"weight\")),\n                    (flax_base + (\"norm1\", \"bias\"), hf_base + (\"layer_norm1\", \"bias\")),\n                    (flax_base + (\"norm2\", \"scale\"), hf_base + (\"layer_norm2\", \"weight\")),\n                    (flax_base + (\"norm2\", \"bias\"), hf_base + (\"layer_norm2\", \"bias\")),\n                    (flax_base + (\"mlp\", \"layers\", 0, \"kernel\"), hf_base + (\"mlp\", \"fc1\", \"weight\")),\n                    (flax_base + (\"mlp\", \"layers\", 0, \"bias\"), hf_base + (\"mlp\", \"fc1\", \"bias\")),\n                    (flax_base + (\"mlp\", \"layers\", 3, \"kernel\"), hf_base + (\"mlp\", \"fc2\", \"weight\")),\n                    (flax_base + (\"mlp\", \"layers\", 3, \"bias\"), hf_base + (\"mlp\", \"fc2\", \"bias\")),\n                ]\n            )\n\n        for i in range(vision_config[\"num_hidden_layers\"]):\n            flax_base = (\"vision_model\", \"transformer\", \"blocks\", \"layers\", i)\n            hf_base = (\"vision_model\", \"encoder\", \"layers\", str(i))\n\n            mapping_list.extend(\n                [\n                    (flax_base + (\"attn\", \"query\", \"kernel\"), hf_base + (\"self_attn\", \"q_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"query\", \"bias\"), hf_base + (\"self_attn\", \"q_proj\", \"bias\")),\n                    (flax_base + (\"attn\", \"key\", \"kernel\"), hf_base + (\"self_attn\", \"k_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"key\", \"bias\"), hf_base + (\"self_attn\", \"k_proj\", \"bias\")),\n                    (flax_base + (\"attn\", \"value\", \"kernel\"), hf_base + (\"self_attn\", \"v_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"value\", \"bias\"), hf_base + (\"self_attn\", \"v_proj\", \"bias\")),\n                    (flax_base + (\"attn\", \"out\", \"kernel\"), hf_base + (\"self_attn\", \"out_proj\", \"weight\")),\n                    (flax_base + (\"attn\", \"out\", \"bias\"), hf_base + (\"self_attn\", \"out_proj\", \"bias\")),\n                    (flax_base + (\"norm1\", \"scale\"), hf_base + (\"layer_norm1\", \"weight\")),\n                    (flax_base + (\"norm1\", \"bias\"), hf_base + (\"layer_norm1\", \"bias\")),\n                    (flax_base + (\"norm2\", \"scale\"), hf_base + (\"layer_norm2\", \"weight\")),\n                    (flax_base + (\"norm2\", \"bias\"), hf_base + (\"layer_norm2\", \"bias\")),\n                    (flax_base + (\"mlp\", \"layers\", 0, \"kernel\"), hf_base + (\"mlp\", \"fc1\", \"weight\")),\n                    (flax_base + (\"mlp\", \"layers\", 0, \"bias\"), hf_base + (\"mlp\", \"fc1\", \"bias\")),\n                    (flax_base + (\"mlp\", \"layers\", 3, \"kernel\"), hf_base + (\"mlp\", \"fc2\", \"weight\")),\n                    (flax_base + (\"mlp\", \"layers\", 3, \"bias\"), hf_base + (\"mlp\", \"fc2\", \"bias\")),\n                ]\n            )\n\n        params_name_mapping = dict(mapping_list)\n        nonvisited = set(flax_model_params_fstate.keys())\n\n        hf_checkpoint_keys: Set[str] = set(params_fstate.keys())\n        used_hf_keys: Set[str] = set()\n\n        for flax_dst_key_tuple, hf_src_key_tuple in params_name_mapping.items():\n            if flax_dst_key_tuple not in flax_model_params_fstate:\n                continue\n\n            hf_src_key_as_string = \".\".join(hf_src_key_tuple)\n            if hf_src_key_as_string not in params_fstate:\n                continue\n\n            used_hf_keys.add(hf_src_key_as_string)\n            nonvisited.discard(flax_dst_key_tuple)\n            src_value = params_fstate[hf_src_key_as_string]\n            dst_value_obj = flax_model_params_fstate[flax_dst_key_tuple]\n            original_param_sharding = dst_value_obj.value.sharding\n\n            if flax_dst_key_tuple == (\"vision_model\", \"conv1\", \"kernel\"):\n                src_value = jnp.transpose(src_value, (2, 3, 1, 0))\n            elif flax_dst_key_tuple == (\"vision_model\", \"cls_token\"):\n                src_value = src_value.reshape(1, 1, -1)\n            elif flax_dst_key_tuple == (\"vision_model\", \"position_embeddings\"):\n                src_value = src_value.reshape(1, src_value.shape[0], src_value.shape[1])\n            elif hf_src_key_tuple[-1] == \"weight\" and hf_src_key_tuple[-2] in (\"q_proj\", \"k_proj\", \"v_proj\"):\n                src_value = jnp.transpose(src_value, (1, 0))\n                if flax_dst_key_tuple[0] == \"text_model\":\n                    num_heads = text_config[\"num_attention_heads\"]\n                    hidden_size = text_config[\"hidden_size\"]\n                else:\n                    num_heads = vision_config[\"hidden_size\"] // 64\n                    hidden_size = vision_config[\"hidden_size\"]\n                head_dim = hidden_size // num_heads\n                src_value = src_value.reshape((hidden_size, num_heads, head_dim))\n            elif hf_src_key_tuple[-1] == \"bias\" and hf_src_key_tuple[-2] in (\"q_proj\", \"k_proj\", \"v_proj\"):\n                if flax_dst_key_tuple[0] == \"text_model\":\n                    num_heads = text_config[\"num_attention_heads\"]\n                    hidden_size = text_config[\"hidden_size\"]\n                else:\n                    num_heads = vision_config[\"hidden_size\"] // 64\n                    hidden_size = vision_config[\"hidden_size\"]\n                head_dim = hidden_size // num_heads\n                src_value = src_value.reshape((num_heads, head_dim))\n            elif hf_src_key_tuple[-2:] == (\"out_proj\", \"weight\"):\n                src_value = jnp.transpose(src_value, (1, 0))\n                if flax_dst_key_tuple[0] == \"text_model\":\n                    num_heads = text_config[\"num_attention_heads\"]\n                    hidden_size = text_config[\"hidden_size\"]\n                else:\n                    num_heads = vision_config[\"hidden_size\"] // 64\n                    hidden_size = vision_config[\"hidden_size\"]\n                head_dim = hidden_size // num_heads\n                src_value = src_value.reshape((num_heads, head_dim, hidden_size))\n            elif flax_dst_key_tuple == (\"token_embedding\", \"embedding\"):\n                pass\n            elif flax_dst_key_tuple == (\"positional_embedding\",):\n                pass\n            elif hf_src_key_tuple[-1] == \"weight\" and src_value.ndim == 2:\n                src_value = jnp.transpose(src_value, (1, 0))\n\n            if src_value.shape != dst_value_obj.value.shape:\n                raise ValueError(f\"Shape mismatch for {flax_dst_key_tuple} (Flax) vs {hf_src_key_as_string} (HF): {dst_value_obj.value.shape} (expected) != {src_value.shape} (actual)\")\n\n            sharded_new_value = jax.device_put(src_value, original_param_sharding)\n            dst_value_obj.value = sharded_new_value\n\n        nnx.update(model, nnx.from_flat_state(flax_model_params_fstate))\n        assert len(nonvisited) == 0, f\"Some Flax CLIP model parameters were not visited: {sorted(list(nonvisited))}\"\n\n        leftover_hf_keys = hf_checkpoint_keys - used_hf_keys\n        known_unused_hf_buffer_keys = {\n            \"text_model.embeddings.position_ids\",\n            \"vision_model.embeddings.position_ids\",\n        }\n        unexpected_leftover_hf_keys = leftover_hf_keys - known_unused_hf_buffer_keys\n\n        assert len(unexpected_leftover_hf_keys) == 0, f\"Some unexpected HuggingFace checkpoint parameters were not used: {sorted(list(unexpected_leftover_hf_keys))}\"\n\n        return model\n</code></pre>"},{"location":"models/CLIP/#jimm.models.clip.CLIP.__call__","title":"<code>__call__(image, text)</code>","text":"<p>Calculate similarity between image and text embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Float[Array, 'batch height width channels']</code> <p>Batch of input images.</p> required <code>text</code> <code>Int[Array, 'batch context_length']</code> <p>Batch of token sequences.</p> required <p>Returns:</p> Type Description <code>Float[Array, 'batch batch']</code> <p>Float[Array, \"batch batch\"]: Similarity scores between all pairs of images and texts.</p> Source code in <code>src/jimm/models/clip.py</code> <pre><code>def __call__(self, image: Float[Array, \"batch height width channels\"], text: Int[Array, \"batch context_length\"]) -&gt; Float[Array, \"batch batch\"]:\n    \"\"\"\n    Calculate similarity between image and text embeddings.\n\n    Args:\n        image (Float[Array, \"batch height width channels\"]): Batch of input images.\n        text (Int[Array, \"batch context_length\"]): Batch of token sequences.\n\n    Returns:\n        Float[Array, \"batch batch\"]: Similarity scores between all pairs of images and texts.\n    \"\"\"\n    image_features: Float[Array, \"batch transformer_width\"] = self.encode_image(image)\n    text_features: Float[Array, \"batch transformer_width\"] = self.encode_text(text)\n\n    image_features: Float[Array, \"batch transformer_width\"] = image_features / jnp.linalg.norm(image_features, axis=-1, keepdims=True)\n    text_features: Float[Array, \"batch transformer_width\"] = text_features / jnp.linalg.norm(text_features, axis=-1, keepdims=True)\n\n    logit_scale: Float[Array, \"\"] = jnp.exp(self.logit_scale.value)\n    logits: Float[Array, \"batch batch\"] = logit_scale * image_features @ text_features.T\n    return logits\n</code></pre>"},{"location":"models/CLIP/#jimm.models.clip.CLIP.__init__","title":"<code>__init__(image_resolution, vision_layers, vision_width, vision_patch_size, context_length, vocab_size, transformer_width, transformer_heads, transformer_layers, rngs=nnx.Rngs(0), dtype=jnp.float32, param_dtype=jnp.float32, mesh=None)</code>","text":"<p>Initialize the CLIP model.</p> <p>Parameters:</p> Name Type Description Default <code>image_resolution</code> <code>int</code> <p>The resolution of the input images.</p> required <code>vision_layers</code> <code>int</code> <p>The number of layers in the vision transformer.</p> required <code>vision_width</code> <code>int</code> <p>The width of the vision transformer.</p> required <code>vision_patch_size</code> <code>int</code> <p>The patch size of the vision transformer.</p> required <code>context_length</code> <code>int</code> <p>The length of the context.</p> required <code>vocab_size</code> <code>int</code> <p>The size of the vocabulary.</p> required <code>transformer_width</code> <code>int</code> <p>The width of the transformer.</p> required <code>transformer_heads</code> <code>int</code> <p>The number of attention heads in the transformer.</p> required <code>transformer_layers</code> <code>int</code> <p>The number of layers in the transformer.</p> required <code>rngs</code> <code>Rngs</code> <p>The random number generator state. Defaults to nnx.Rngs(0).</p> <code>Rngs(0)</code> <code>dtype</code> <code>DTypeLike</code> <p>The data type for computations. Defaults to jnp.float32.</p> <code>float32</code> <code>param_dtype</code> <code>DTypeLike</code> <p>The data type for parameters. Defaults to jnp.float32.</p> <code>float32</code> <code>mesh</code> <code>Mesh | None</code> <p>The device mesh for parameter sharding. Defaults to None.</p> <code>None</code> Source code in <code>src/jimm/models/clip.py</code> <pre><code>def __init__(\n    self,\n    image_resolution: int,\n    vision_layers: int,\n    vision_width: int,\n    vision_patch_size: int,\n    context_length: int,\n    vocab_size: int,\n    transformer_width: int,\n    transformer_heads: int,\n    transformer_layers: int,\n    rngs: nnx.Rngs = nnx.Rngs(0),\n    dtype: DTypeLike = jnp.float32,\n    param_dtype: DTypeLike = jnp.float32,\n    mesh: Mesh | None = None,\n):\n    \"\"\"\n    Initialize the CLIP model.\n\n    Args:\n        image_resolution (int): The resolution of the input images.\n        vision_layers (int): The number of layers in the vision transformer.\n        vision_width (int): The width of the vision transformer.\n        vision_patch_size (int): The patch size of the vision transformer.\n        context_length (int): The length of the context.\n        vocab_size (int): The size of the vocabulary.\n        transformer_width (int): The width of the transformer.\n        transformer_heads (int): The number of attention heads in the transformer.\n        transformer_layers (int): The number of layers in the transformer.\n        rngs (nnx.Rngs): The random number generator state. Defaults to nnx.Rngs(0).\n        dtype (DTypeLike): The data type for computations. Defaults to jnp.float32.\n        param_dtype (DTypeLike): The data type for parameters. Defaults to jnp.float32.\n        mesh (Mesh | None): The device mesh for parameter sharding. Defaults to None.\n    \"\"\"\n    self.vision_layers = vision_layers\n    self.vision_width = vision_width\n    self.vision_patch_size = vision_patch_size\n    self.context_length = context_length\n    self.vocab_size = vocab_size\n    self.transformer_width = transformer_width\n    self.transformer_heads = transformer_heads\n    self.transformer_layers = transformer_layers\n    self.dtype = dtype\n\n    vision_heads = vision_width // 64\n\n    self.attn_mask: Float[Array, \"context_length context_length\"] = jnp.tril(jnp.ones((context_length, context_length), dtype=dtype))\n\n    self.vision_model = CLIPVisionTransformer(\n        input_resolution=image_resolution,\n        patch_size=vision_patch_size,\n        width=vision_width,\n        layers=vision_layers,\n        num_heads=vision_heads,\n        output_dim=transformer_width,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        mesh=mesh,\n        rngs=rngs,\n    )\n\n    self.text_model = Transformer(\n        width=transformer_width,\n        mlp_dim=transformer_width * 4,\n        layers=transformer_layers,\n        num_heads=transformer_heads,\n        dropout_rate=0.0,\n        attn_mask=self.attn_mask,\n        use_quick_gelu=True,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        mesh=mesh,\n        rngs=rngs,\n    )\n    self.vocab_size = vocab_size\n    self.token_embedding = nnx.Embed(\n        num_embeddings=vocab_size,\n        features=transformer_width,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        rngs=rngs,\n        embedding_init=sharded_init(nnx.initializers.xavier_uniform(), P(\"model\", None), mesh),\n    )\n    self.positional_embedding = nnx.Param(sharded_init(nnx.initializers.truncated_normal(stddev=0.02), P(\"model\", None), mesh)(rngs.params(), (context_length, transformer_width)))\n    self.ln_final = nnx.LayerNorm(\n        transformer_width,\n        epsilon=1e-5,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        rngs=rngs,\n        scale_init=sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh),\n        bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n    )\n    self.text_projection = nnx.Linear(\n        transformer_width,\n        transformer_width,\n        use_bias=False,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        rngs=rngs,\n        kernel_init=sharded_init(nnx.initializers.xavier_uniform(), P(\"model\", None), mesh),\n    )\n    self.logit_scale = nnx.Param(sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh)(rngs.params(), ()))\n</code></pre>"},{"location":"models/CLIP/#jimm.models.clip.CLIP.encode_image","title":"<code>encode_image(image)</code>","text":"<p>Encode images into embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Float[Array, 'batch height width channels']</code> <p>Batch of input images.</p> required <p>Returns:</p> Type Description <code>Float[Array, 'batch transformer_width']</code> <p>Float[Array, \"batch transformer_width\"]: Image embeddings.</p> Source code in <code>src/jimm/models/clip.py</code> <pre><code>def encode_image(self, image: Float[Array, \"batch height width channels\"]) -&gt; Float[Array, \"batch transformer_width\"]:\n    \"\"\"\n    Encode images into embeddings.\n\n    Args:\n        image (Float[Array, \"batch height width channels\"]): Batch of input images.\n\n    Returns:\n        Float[Array, \"batch transformer_width\"]: Image embeddings.\n    \"\"\"\n    return self.vision_model(image)\n</code></pre>"},{"location":"models/CLIP/#jimm.models.clip.CLIP.encode_text","title":"<code>encode_text(text)</code>","text":"<p>Encode text tokens into embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Int[Array, 'batch context_length']</code> <p>Batch of token sequences.</p> required <p>Returns:</p> Type Description <code>Float[Array, 'batch transformer_width']</code> <p>Float[Array, \"batch transformer_width\"]: Text embeddings.</p> Source code in <code>src/jimm/models/clip.py</code> <pre><code>def encode_text(self, text: Int[Array, \"batch context_length\"]) -&gt; Float[Array, \"batch transformer_width\"]:\n    \"\"\"\n    Encode text tokens into embeddings.\n\n    Args:\n        text (Int[Array, \"batch context_length\"]): Batch of token sequences.\n\n    Returns:\n        Float[Array, \"batch transformer_width\"]: Text embeddings.\n    \"\"\"\n    seq_len = text.shape[1]\n    x: Float[Array, \"batch context_length transformer_width\"] = self.token_embedding(text)\n    x: Float[Array, \"batch context_length transformer_width\"] = x + self.positional_embedding.value[:seq_len]\n    x: Float[Array, \"batch context_length transformer_width\"] = self.text_model(x)\n    x: Float[Array, \"batch context_length transformer_width\"] = self.ln_final(x)\n\n    eot_token_pos: Float[Array, \" batch \"] = jnp.argmax(text, axis=-1)\n    batch_indices: Float[Array, \" batch \"] = jnp.arange(x.shape[0])\n    x: Float[Array, \"batch transformer_width\"] = x[batch_indices, eot_token_pos] @ self.text_projection.kernel.value\n    return x\n</code></pre>"},{"location":"models/CLIP/#jimm.models.clip.CLIP.from_pretrained","title":"<code>from_pretrained(model_name_or_path, use_pytorch=False, mesh=None, dtype=jnp.float32)</code>  <code>classmethod</code>","text":"<p>Load a pretrained CLIP model from a local path or HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Path to local weights or HuggingFace model ID.</p> required <code>use_pytorch</code> <code>bool</code> <p>Whether to load from PyTorch weights. Defaults to False.</p> <code>False</code> <code>mesh</code> <code>Mesh | None</code> <p>Optional device mesh for parameter sharding. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>DTypeLike</code> <p>Data type for computations. Defaults to jnp.float32.</p> <code>float32</code> <p>Returns:</p> Name Type Description <code>CLIP</code> <code>CLIP</code> <p>Pretrained CLIP model</p> Source code in <code>src/jimm/models/clip.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, model_name_or_path: str, use_pytorch: bool = False, mesh: Mesh | None = None, dtype: DTypeLike = jnp.float32) -&gt; \"CLIP\":\n    \"\"\"Load a pretrained CLIP model from a local path or HuggingFace Hub.\n\n    Args:\n        model_name_or_path (str): Path to local weights or HuggingFace model ID.\n        use_pytorch (bool): Whether to load from PyTorch weights. Defaults to False.\n        mesh (Mesh|None): Optional device mesh for parameter sharding. Defaults to None.\n        dtype (DTypeLike): Data type for computations. Defaults to jnp.float32.\n\n    Returns:\n        CLIP: Pretrained CLIP model\n    \"\"\"\n    import jax\n\n    params_fstate, config_dict = load_params_and_config(model_name_or_path, use_pytorch)\n\n    config: dict[str, Any] | None = config_dict\n\n    if config is None:\n        if not use_pytorch:\n            text_hidden_size = params_fstate[\"text_model.embeddings.token_embedding.weight\"].shape[1]\n            text_max_pos_embed = params_fstate[\"text_model.embeddings.position_embedding.weight\"].shape[0]\n            text_vocab_size = params_fstate[\"text_model.embeddings.token_embedding.weight\"].shape[0]\n\n            text_num_layers = 0\n            for k_param in params_fstate:\n                if k_param.startswith(\"text_model.encoder.layers.\") and k_param.endswith(\".self_attn.q_proj.weight\"):\n                    layer_idx = int(k_param.split(\".\")[3])\n                    text_num_layers = max(text_num_layers, layer_idx + 1)\n\n            vision_hidden_size = params_fstate[\"vision_model.embeddings.class_embedding\"].shape[0]\n            vision_patch_size = params_fstate[\"vision_model.embeddings.patch_embedding.weight\"].shape[2]\n            vision_image_size = int((params_fstate[\"vision_model.embeddings.position_embedding.weight\"].shape[0] - 1) ** 0.5) * vision_patch_size\n\n            vision_num_layers = 0\n            for k_param in params_fstate:\n                if k_param.startswith(\"vision_model.encoder.layers.\") and k_param.endswith(\".self_attn.q_proj.weight\"):\n                    layer_idx = int(k_param.split(\".\")[3])\n                    vision_num_layers = max(vision_num_layers, layer_idx + 1)\n\n            config = {\n                \"text_config\": {\n                    \"hidden_size\": text_hidden_size,\n                    \"num_attention_heads\": text_hidden_size // 64,\n                    \"num_hidden_layers\": text_num_layers,\n                    \"max_position_embeddings\": text_max_pos_embed,\n                    \"vocab_size\": text_vocab_size,\n                },\n                \"vision_config\": {\n                    \"hidden_size\": vision_hidden_size,\n                    \"num_attention_heads\": vision_hidden_size // 64,\n                    \"num_hidden_layers\": vision_num_layers,\n                    \"image_size\": vision_image_size,\n                    \"patch_size\": vision_patch_size,\n                },\n            }\n        else:\n            raise ValueError(f\"Configuration could not be loaded for PyTorch model {model_name_or_path}\")\n\n    text_config = config[\"text_config\"]\n    vision_config = config[\"vision_config\"]\n\n    model = cls(\n        image_resolution=vision_config[\"image_size\"],\n        vision_layers=vision_config[\"num_hidden_layers\"],\n        vision_width=vision_config[\"hidden_size\"],\n        vision_patch_size=vision_config[\"patch_size\"],\n        context_length=text_config[\"max_position_embeddings\"],\n        vocab_size=text_config[\"vocab_size\"],\n        transformer_width=text_config[\"hidden_size\"],\n        transformer_heads=text_config[\"num_attention_heads\"],\n        transformer_layers=text_config[\"num_hidden_layers\"],\n        mesh=mesh,\n        dtype=dtype,\n        param_dtype=dtype,\n    )\n\n    flax_model_params_fstate = dict(nnx.to_flat_state(nnx.state(model, nnx.Param)))\n\n    mapping_list = [\n        ((\"logit_scale\",), (\"logit_scale\",)),\n        ((\"positional_embedding\",), (\"text_model\", \"embeddings\", \"position_embedding\", \"weight\")),\n        ((\"token_embedding\", \"embedding\"), (\"text_model\", \"embeddings\", \"token_embedding\", \"weight\")),\n        ((\"ln_final\", \"scale\"), (\"text_model\", \"final_layer_norm\", \"weight\")),\n        ((\"ln_final\", \"bias\"), (\"text_model\", \"final_layer_norm\", \"bias\")),\n        ((\"text_projection\", \"kernel\"), (\"text_projection\", \"weight\")),\n        ((\"vision_model\", \"cls_token\"), (\"vision_model\", \"embeddings\", \"class_embedding\")),\n        ((\"vision_model\", \"position_embeddings\"), (\"vision_model\", \"embeddings\", \"position_embedding\", \"weight\")),\n        ((\"vision_model\", \"conv1\", \"kernel\"), (\"vision_model\", \"embeddings\", \"patch_embedding\", \"weight\")),\n        ((\"vision_model\", \"ln_pre\", \"scale\"), (\"vision_model\", \"pre_layrnorm\", \"weight\")),\n        ((\"vision_model\", \"ln_pre\", \"bias\"), (\"vision_model\", \"pre_layrnorm\", \"bias\")),\n        ((\"vision_model\", \"ln_post\", \"scale\"), (\"vision_model\", \"post_layernorm\", \"weight\")),\n        ((\"vision_model\", \"ln_post\", \"bias\"), (\"vision_model\", \"post_layernorm\", \"bias\")),\n        ((\"vision_model\", \"proj\", \"kernel\"), (\"visual_projection\", \"weight\")),\n    ]\n\n    for i in range(text_config[\"num_hidden_layers\"]):\n        flax_base = (\"text_model\", \"blocks\", \"layers\", i)\n        hf_base = (\"text_model\", \"encoder\", \"layers\", str(i))\n\n        mapping_list.extend(\n            [\n                (flax_base + (\"attn\", \"query\", \"kernel\"), hf_base + (\"self_attn\", \"q_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"query\", \"bias\"), hf_base + (\"self_attn\", \"q_proj\", \"bias\")),\n                (flax_base + (\"attn\", \"key\", \"kernel\"), hf_base + (\"self_attn\", \"k_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"key\", \"bias\"), hf_base + (\"self_attn\", \"k_proj\", \"bias\")),\n                (flax_base + (\"attn\", \"value\", \"kernel\"), hf_base + (\"self_attn\", \"v_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"value\", \"bias\"), hf_base + (\"self_attn\", \"v_proj\", \"bias\")),\n                (flax_base + (\"attn\", \"out\", \"kernel\"), hf_base + (\"self_attn\", \"out_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"out\", \"bias\"), hf_base + (\"self_attn\", \"out_proj\", \"bias\")),\n                (flax_base + (\"norm1\", \"scale\"), hf_base + (\"layer_norm1\", \"weight\")),\n                (flax_base + (\"norm1\", \"bias\"), hf_base + (\"layer_norm1\", \"bias\")),\n                (flax_base + (\"norm2\", \"scale\"), hf_base + (\"layer_norm2\", \"weight\")),\n                (flax_base + (\"norm2\", \"bias\"), hf_base + (\"layer_norm2\", \"bias\")),\n                (flax_base + (\"mlp\", \"layers\", 0, \"kernel\"), hf_base + (\"mlp\", \"fc1\", \"weight\")),\n                (flax_base + (\"mlp\", \"layers\", 0, \"bias\"), hf_base + (\"mlp\", \"fc1\", \"bias\")),\n                (flax_base + (\"mlp\", \"layers\", 3, \"kernel\"), hf_base + (\"mlp\", \"fc2\", \"weight\")),\n                (flax_base + (\"mlp\", \"layers\", 3, \"bias\"), hf_base + (\"mlp\", \"fc2\", \"bias\")),\n            ]\n        )\n\n    for i in range(vision_config[\"num_hidden_layers\"]):\n        flax_base = (\"vision_model\", \"transformer\", \"blocks\", \"layers\", i)\n        hf_base = (\"vision_model\", \"encoder\", \"layers\", str(i))\n\n        mapping_list.extend(\n            [\n                (flax_base + (\"attn\", \"query\", \"kernel\"), hf_base + (\"self_attn\", \"q_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"query\", \"bias\"), hf_base + (\"self_attn\", \"q_proj\", \"bias\")),\n                (flax_base + (\"attn\", \"key\", \"kernel\"), hf_base + (\"self_attn\", \"k_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"key\", \"bias\"), hf_base + (\"self_attn\", \"k_proj\", \"bias\")),\n                (flax_base + (\"attn\", \"value\", \"kernel\"), hf_base + (\"self_attn\", \"v_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"value\", \"bias\"), hf_base + (\"self_attn\", \"v_proj\", \"bias\")),\n                (flax_base + (\"attn\", \"out\", \"kernel\"), hf_base + (\"self_attn\", \"out_proj\", \"weight\")),\n                (flax_base + (\"attn\", \"out\", \"bias\"), hf_base + (\"self_attn\", \"out_proj\", \"bias\")),\n                (flax_base + (\"norm1\", \"scale\"), hf_base + (\"layer_norm1\", \"weight\")),\n                (flax_base + (\"norm1\", \"bias\"), hf_base + (\"layer_norm1\", \"bias\")),\n                (flax_base + (\"norm2\", \"scale\"), hf_base + (\"layer_norm2\", \"weight\")),\n                (flax_base + (\"norm2\", \"bias\"), hf_base + (\"layer_norm2\", \"bias\")),\n                (flax_base + (\"mlp\", \"layers\", 0, \"kernel\"), hf_base + (\"mlp\", \"fc1\", \"weight\")),\n                (flax_base + (\"mlp\", \"layers\", 0, \"bias\"), hf_base + (\"mlp\", \"fc1\", \"bias\")),\n                (flax_base + (\"mlp\", \"layers\", 3, \"kernel\"), hf_base + (\"mlp\", \"fc2\", \"weight\")),\n                (flax_base + (\"mlp\", \"layers\", 3, \"bias\"), hf_base + (\"mlp\", \"fc2\", \"bias\")),\n            ]\n        )\n\n    params_name_mapping = dict(mapping_list)\n    nonvisited = set(flax_model_params_fstate.keys())\n\n    hf_checkpoint_keys: Set[str] = set(params_fstate.keys())\n    used_hf_keys: Set[str] = set()\n\n    for flax_dst_key_tuple, hf_src_key_tuple in params_name_mapping.items():\n        if flax_dst_key_tuple not in flax_model_params_fstate:\n            continue\n\n        hf_src_key_as_string = \".\".join(hf_src_key_tuple)\n        if hf_src_key_as_string not in params_fstate:\n            continue\n\n        used_hf_keys.add(hf_src_key_as_string)\n        nonvisited.discard(flax_dst_key_tuple)\n        src_value = params_fstate[hf_src_key_as_string]\n        dst_value_obj = flax_model_params_fstate[flax_dst_key_tuple]\n        original_param_sharding = dst_value_obj.value.sharding\n\n        if flax_dst_key_tuple == (\"vision_model\", \"conv1\", \"kernel\"):\n            src_value = jnp.transpose(src_value, (2, 3, 1, 0))\n        elif flax_dst_key_tuple == (\"vision_model\", \"cls_token\"):\n            src_value = src_value.reshape(1, 1, -1)\n        elif flax_dst_key_tuple == (\"vision_model\", \"position_embeddings\"):\n            src_value = src_value.reshape(1, src_value.shape[0], src_value.shape[1])\n        elif hf_src_key_tuple[-1] == \"weight\" and hf_src_key_tuple[-2] in (\"q_proj\", \"k_proj\", \"v_proj\"):\n            src_value = jnp.transpose(src_value, (1, 0))\n            if flax_dst_key_tuple[0] == \"text_model\":\n                num_heads = text_config[\"num_attention_heads\"]\n                hidden_size = text_config[\"hidden_size\"]\n            else:\n                num_heads = vision_config[\"hidden_size\"] // 64\n                hidden_size = vision_config[\"hidden_size\"]\n            head_dim = hidden_size // num_heads\n            src_value = src_value.reshape((hidden_size, num_heads, head_dim))\n        elif hf_src_key_tuple[-1] == \"bias\" and hf_src_key_tuple[-2] in (\"q_proj\", \"k_proj\", \"v_proj\"):\n            if flax_dst_key_tuple[0] == \"text_model\":\n                num_heads = text_config[\"num_attention_heads\"]\n                hidden_size = text_config[\"hidden_size\"]\n            else:\n                num_heads = vision_config[\"hidden_size\"] // 64\n                hidden_size = vision_config[\"hidden_size\"]\n            head_dim = hidden_size // num_heads\n            src_value = src_value.reshape((num_heads, head_dim))\n        elif hf_src_key_tuple[-2:] == (\"out_proj\", \"weight\"):\n            src_value = jnp.transpose(src_value, (1, 0))\n            if flax_dst_key_tuple[0] == \"text_model\":\n                num_heads = text_config[\"num_attention_heads\"]\n                hidden_size = text_config[\"hidden_size\"]\n            else:\n                num_heads = vision_config[\"hidden_size\"] // 64\n                hidden_size = vision_config[\"hidden_size\"]\n            head_dim = hidden_size // num_heads\n            src_value = src_value.reshape((num_heads, head_dim, hidden_size))\n        elif flax_dst_key_tuple == (\"token_embedding\", \"embedding\"):\n            pass\n        elif flax_dst_key_tuple == (\"positional_embedding\",):\n            pass\n        elif hf_src_key_tuple[-1] == \"weight\" and src_value.ndim == 2:\n            src_value = jnp.transpose(src_value, (1, 0))\n\n        if src_value.shape != dst_value_obj.value.shape:\n            raise ValueError(f\"Shape mismatch for {flax_dst_key_tuple} (Flax) vs {hf_src_key_as_string} (HF): {dst_value_obj.value.shape} (expected) != {src_value.shape} (actual)\")\n\n        sharded_new_value = jax.device_put(src_value, original_param_sharding)\n        dst_value_obj.value = sharded_new_value\n\n    nnx.update(model, nnx.from_flat_state(flax_model_params_fstate))\n    assert len(nonvisited) == 0, f\"Some Flax CLIP model parameters were not visited: {sorted(list(nonvisited))}\"\n\n    leftover_hf_keys = hf_checkpoint_keys - used_hf_keys\n    known_unused_hf_buffer_keys = {\n        \"text_model.embeddings.position_ids\",\n        \"vision_model.embeddings.position_ids\",\n    }\n    unexpected_leftover_hf_keys = leftover_hf_keys - known_unused_hf_buffer_keys\n\n    assert len(unexpected_leftover_hf_keys) == 0, f\"Some unexpected HuggingFace checkpoint parameters were not used: {sorted(list(unexpected_leftover_hf_keys))}\"\n\n    return model\n</code></pre>"},{"location":"models/ViT/","title":"ViT (Vision Transformer)","text":"<p>The ViT (Vision Transformer) is a transformer-based neural network architecture for image classification. It divides an image into fixed-size patches, linearly embeds each patch, adds position embeddings, and processes the resulting sequence of vectors through a standard transformer encoder.</p> <p>The ViT model was introduced in the paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" and has shown strong performance on image classification benchmarks.</p>"},{"location":"models/ViT/#jimm.models.vit.VisionTransformer","title":"<code>jimm.models.vit.VisionTransformer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Vision Transformer (ViT) model for image classification.</p> <p>This implements the Vision Transformer as described in the paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"</p> Source code in <code>src/jimm/models/vit.py</code> <pre><code>class VisionTransformer(nnx.Module):\n    \"\"\"Vision Transformer (ViT) model for image classification.\n\n    This implements the Vision Transformer as described in the paper\n    \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"\n    \"\"\"\n\n    def __init__(\n        self,\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        img_size: int = 224,\n        patch_size: int = 16,\n        num_layers: int = 12,\n        num_heads: int = 12,\n        mlp_dim: int = 3072,\n        hidden_size: int = 768,\n        dropout_rate: float = 0.1,\n        use_quick_gelu: bool = False,\n        dtype: DTypeLike = jnp.float32,\n        param_dtype: DTypeLike = jnp.float32,\n        rngs: nnx.Rngs = nnx.Rngs(0),\n        mesh: Mesh | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize a Vision Transformer.\n\n        Args:\n            num_classes (int): Number of output classes. Defaults to 1000.\n            in_channels (int): Number of input channels. Defaults to 3.\n            img_size (int): Size of the input image (assumed square). Defaults to 224.\n            patch_size (int): Size of each patch (assumed square). Defaults to 16.\n            num_layers (int): Number of transformer layers. Defaults to 12.\n            num_heads (int): Number of attention heads. Defaults to 12.\n            mlp_dim (int): Size of the MLP dimension. Defaults to 3072.\n            hidden_size (int): Size of the hidden dimension. Defaults to 768.\n            dropout_rate (float): Dropout rate. Defaults to 0.1.\n            use_quick_gelu (bool): Whether to use quickgelu instead of gelu. Defaults to False.\n            dtype (DTypeLike): Data type for computations. Defaults to jnp.float32.\n            param_dtype (DTypeLike): Data type for parameters. Defaults to jnp.float32.\n            rngs (nnx.Rngs): Random number generator keys. Defaults to nnx.Rngs(0).\n            mesh (Mesh|None): Optional JAX device mesh for parameter sharding. Defaults to None.\n        \"\"\"\n        n_patches: int = (img_size // patch_size) ** 2\n        self.patch_embeddings = nnx.Conv(\n            in_features=in_channels,\n            out_features=hidden_size,\n            kernel_size=(patch_size, patch_size),\n            strides=(patch_size, patch_size),\n            padding=\"VALID\",\n            use_bias=True,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            rngs=rngs,\n            kernel_init=sharded_init(nnx.initializers.xavier_uniform(), P(None, None, None, \"model\"), mesh),\n            bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n        )\n        _position_embeddings_initializer = sharded_init(nnx.initializers.truncated_normal(stddev=0.02), P(None, None, \"model\"), mesh)\n        pos_emb_value: Float[Array, \"one n_patches_plus_1 hidden_size_dim\"] = _position_embeddings_initializer(rngs.params(), (1, n_patches + 1, hidden_size))\n        self.position_embeddings = nnx.Param(pos_emb_value)\n\n        self.dropout = nnx.Dropout(dropout_rate, rngs=rngs)\n\n        _cls_token_initializer = sharded_init(nnx.initializers.zeros_init(), P(None, None, \"model\"), mesh)\n        cls_token_value: Float[Array, \"one one hidden_size_dim\"] = _cls_token_initializer(rngs.params(), (1, 1, hidden_size))\n        self.cls_token = nnx.Param(cls_token_value)\n\n        self.encoder = Transformer(\n            width=hidden_size,\n            mlp_dim=mlp_dim,\n            layers=num_layers,\n            num_heads=num_heads,\n            dropout_rate=dropout_rate,\n            use_quick_gelu=use_quick_gelu,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            rngs=rngs,\n            mesh=mesh,\n        )\n\n        self.final_norm = nnx.LayerNorm(\n            hidden_size,\n            epsilon=1e-12,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            rngs=rngs,\n            scale_init=sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh),\n            bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n        )\n        self.classifier = nnx.Linear(\n            hidden_size,\n            num_classes,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            rngs=rngs,\n            kernel_init=sharded_init(nnx.initializers.xavier_uniform(), P(None, \"model\"), mesh),\n            bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n        )\n\n    def __call__(self, x: Float[Array, \"batch height width channels\"]) -&gt; Float[Array, \"batch num_classes\"]:\n        \"\"\"Forward pass of the Vision Transformer.\n\n        Args:\n            x (Float[Array, \"batch height width channels\"]): Input tensor with shape [batch, height, width, channels]\n\n        Returns:\n            Float[Array, \"batch num_classes\"]: Output logits with shape [batch, num_classes]\n        \"\"\"\n        patches = self.patch_embeddings(x)\n        batch_size = patches.shape[0]\n        patches = patches.reshape(batch_size, -1, patches.shape[-1])\n        cls_token = jnp.tile(self.cls_token.value, [batch_size, 1, 1])\n        x = jnp.concat([cls_token, patches], axis=1)\n        embeddings = x + self.position_embeddings.value\n        embeddings = self.dropout(embeddings)\n        x = self.encoder(embeddings)\n        x = self.final_norm(x)\n        x = x[:, 0]\n        return self.classifier(x)\n\n    @classmethod\n    def from_pretrained(cls, model_name_or_path: str, use_pytorch: bool = False, mesh: Mesh | None = None, dtype: DTypeLike = jnp.float32) -&gt; \"VisionTransformer\":\n        \"\"\"Load a pretrained Vision Transformer from a local path or HuggingFace Hub.\n\n        Args:\n            model_name_or_path (str): Path to local weights or HuggingFace model ID.\n            use_pytorch (bool): Whether to load from PyTorch weights. Defaults to False.\n            mesh (Mesh|None): Optional device mesh for parameter sharding. Defaults to None.\n            dtype (DTypeLike): Data type for computations. Defaults to jnp.float32.\n\n        Returns:\n            VisionTransformer: Initialized Vision Transformer with pretrained weights\n        \"\"\"\n        params_fstate, config_dict = load_params_and_config(model_name_or_path, use_pytorch)\n\n        config: dict[str, Any] | None = config_dict\n\n        hidden_size_val: int\n        num_classes_val: int\n        num_layers_val: int\n        num_heads_val: int\n        mlp_dim_val: int\n        patch_size_val: int\n        img_size_val: int\n        use_quick_gelu_val: bool = False\n\n        if config:\n            hidden_size_val = config[\"hidden_size\"]\n            num_classes_val = len(config[\"id2label\"]) if \"id2label\" in config else config.get(\"num_labels\", 1000)\n            num_layers_val = config[\"num_hidden_layers\"]\n            num_heads_val = config[\"num_attention_heads\"]\n            mlp_dim_val = config[\"intermediate_size\"]\n            patch_size_val = config[\"patch_size\"]\n            img_size_val = config[\"image_size\"]\n            if \"hidden_act\" in config and config[\"hidden_act\"] == \"quick_gelu\":\n                use_quick_gelu_val = True\n            elif \"hidden_act\" in config and config[\"hidden_act\"] != \"gelu\":\n                print(f\"Warning: Unexpected hidden_act '{config['hidden_act']}' in config, defaulting to standard GELU.\")\n\n        elif not use_pytorch and (os.path.exists(model_name_or_path) and os.path.isfile(model_name_or_path)):\n            hidden_size_val = params_fstate[\"vit.embeddings.cls_token\"].shape[-1]\n            num_classes_val = params_fstate[\"classifier.bias\"].shape[0]\n\n            max_layer_idx = -1\n            for k in params_fstate:\n                if k.startswith(\"vit.encoder.layer.\"):\n                    max_layer_idx = max(max_layer_idx, int(k.split(\".\")[3]))\n            num_layers_val = max_layer_idx + 1\n\n            mlp_dim_val = params_fstate[\"vit.encoder.layer.0.intermediate.dense.weight\"].shape[0]\n\n            assumed_head_dim = 64\n            num_heads_val = hidden_size_val // assumed_head_dim\n\n            patch_kernel_shape = params_fstate[\"vit.embeddings.patch_embeddings.projection.weight\"].shape\n            patch_size_val = patch_kernel_shape[2]\n\n            num_patches_from_embeddings = params_fstate[\"vit.embeddings.position_embeddings\"].shape[1] - 1\n            img_size_dim = int(jnp.sqrt(num_patches_from_embeddings))\n            img_size_val = img_size_dim * patch_size_val\n        else:\n            raise ValueError(f\"Could not load or infer configuration for {model_name_or_path}\")\n\n        if not all(v is not None for v in [hidden_size_val, num_classes_val, num_layers_val, num_heads_val, mlp_dim_val, patch_size_val, img_size_val]):\n            raise ValueError(f\"One or more configuration parameters could not be determined for {model_name_or_path}\")\n\n        model = cls(\n            num_classes=num_classes_val,\n            img_size=img_size_val,\n            patch_size=patch_size_val,\n            num_layers=num_layers_val,\n            num_heads=num_heads_val,\n            mlp_dim=mlp_dim_val,\n            hidden_size=hidden_size_val,\n            use_quick_gelu=use_quick_gelu_val,\n            mesh=mesh,\n            dtype=dtype,\n            param_dtype=dtype,\n        )\n\n        flax_model_params_fstate = dict(nnx.to_flat_state(nnx.state(model, nnx.Param)))\n\n        def hf_param_name(name: str) -&gt; str:\n            return \"weight\" if name in [\"kernel\", \"scale\"] else name\n\n        hidden_size_per_head = hidden_size_val // num_heads_val\n\n        mapping_list = [\n            ((\"cls_token\",), (\"vit\", \"embeddings\", \"cls_token\")),\n            ((\"position_embeddings\",), (\"vit\", \"embeddings\", \"position_embeddings\")),\n            ((\"patch_embeddings\", \"kernel\"), (\"vit\", \"embeddings\", \"patch_embeddings\", \"projection\", \"weight\")),\n            ((\"patch_embeddings\", \"bias\"), (\"vit\", \"embeddings\", \"patch_embeddings\", \"projection\", \"bias\")),\n            ((\"classifier\", \"kernel\"), (\"classifier\", \"weight\")),\n            ((\"classifier\", \"bias\"), (\"classifier\", \"bias\")),\n            ((\"final_norm\", \"scale\"), (\"vit\", \"layernorm\", \"weight\")),\n            ((\"final_norm\", \"bias\"), (\"vit\", \"layernorm\", \"bias\")),\n        ]\n\n        for i in range(num_layers_val):\n            flax_base = (\"encoder\", \"blocks\", \"layers\", i)\n            hf_base = (\"vit\", \"encoder\", \"layer\", str(i))\n            mapping_list.extend(\n                [(flax_base + (\"attn\", y_type, p_name), hf_base + (\"attention\", \"attention\", y_type, hf_param_name(p_name))) for p_name in [\"kernel\", \"bias\"] for y_type in [\"key\", \"value\", \"query\"]]\n            )\n            mapping_list.extend([(flax_base + (\"attn\", \"out\", p_name), hf_base + (\"attention\", \"output\", \"dense\", hf_param_name(p_name))) for p_name in [\"kernel\", \"bias\"]])\n            mapping_list.extend(\n                [\n                    (flax_base + (\"mlp\", \"layers\", y1_idx, p_name), hf_base + (y2_name, \"dense\", hf_param_name(p_name)))\n                    for p_name in [\"kernel\", \"bias\"]\n                    for y1_idx, y2_name in [(0, \"intermediate\"), (3, \"output\")]\n                ]\n            )\n            mapping_list.extend(\n                [\n                    (flax_base + (norm_flax, p_name), hf_base + (norm_hf, hf_param_name(p_name)))\n                    for p_name in [\"scale\", \"bias\"]\n                    for norm_flax, norm_hf in [(\"norm1\", \"layernorm_before\"), (\"norm2\", \"layernorm_after\")]\n                ]\n            )\n        params_name_mapping = dict(mapping_list)\n        nonvisited = set(flax_model_params_fstate.keys())\n        used_hf_keys: Set[str] = set()\n\n        for flax_dst_key_tuple, hf_src_key_tuple in params_name_mapping.items():\n            assert flax_dst_key_tuple in flax_model_params_fstate, flax_dst_key_tuple\n            hf_src_key_as_string = \".\".join(hf_src_key_tuple)\n            used_hf_keys.add(hf_src_key_as_string)\n            assert hf_src_key_as_string in params_fstate, f\"HF key '{hf_src_key_as_string}' (from Flax key {flax_dst_key_tuple}) not found in loaded weights.\"\n            nonvisited.remove(flax_dst_key_tuple)\n            src_value: Array = params_fstate[hf_src_key_as_string]\n\n            dst_value_obj = flax_model_params_fstate[flax_dst_key_tuple]\n            original_param_sharding = dst_value_obj.value.sharding\n\n            if flax_dst_key_tuple == (\"patch_embeddings\", \"kernel\"):\n                src_value = jnp.transpose(src_value, (2, 3, 1, 0))\n            elif hf_src_key_tuple[-1] == \"weight\" and hf_src_key_tuple[-2] in (\"key\", \"value\", \"query\"):\n                src_value = jnp.transpose(src_value, (1, 0))\n                src_value = src_value.reshape((hidden_size_val, num_heads_val, hidden_size_per_head))\n            elif hf_src_key_tuple[-1] == \"bias\" and hf_src_key_tuple[-2] in (\"key\", \"value\", \"query\"):\n                src_value = src_value.reshape((num_heads_val, hidden_size_per_head))\n            elif hf_src_key_tuple[-4:] == (\"attention\", \"output\", \"dense\", \"weight\"):\n                src_value = jnp.transpose(src_value, (1, 0))\n                src_value = src_value.reshape((num_heads_val, hidden_size_per_head, hidden_size_val))\n            elif hf_src_key_tuple[-1] == \"weight\" and src_value.ndim == 2:\n                src_value = jnp.transpose(src_value, (1, 0))\n\n            assert src_value.shape == dst_value_obj.value.shape, f\"Shape mismatch for {flax_dst_key_tuple} (Flax) vs {hf_src_key_as_string} (HF): {dst_value_obj.value.shape} != {src_value.shape}\"\n\n            sharded_new_value: Array = jax.device_put(src_value, original_param_sharding)\n            dst_value_obj.value = sharded_new_value\n\n            assert jnp.allclose(dst_value_obj.value.mean(), src_value.mean()), (dst_value_obj.value.mean(), src_value.mean())\n\n        assert len(nonvisited) == 0, f\"Some Flax model parameters were not visited: {nonvisited}\"\n\n        leftover_hf_keys = set(params_fstate.keys()) - used_hf_keys\n        known_unused_hf_buffer_keys = {\n            \"text_model.embeddings.position_ids\",\n            \"vision_model.embeddings.position_ids\",\n        }\n        unexpected_leftover_hf_keys = leftover_hf_keys - known_unused_hf_buffer_keys\n\n        assert len(unexpected_leftover_hf_keys) == 0, f\"Some unexpected HuggingFace checkpoint parameters were not used: {sorted(list(unexpected_leftover_hf_keys))}\"\n        nnx.update(model, nnx.from_flat_state(flax_model_params_fstate))\n\n        del flax_model_params_fstate\n        del params_fstate\n        return model\n</code></pre>"},{"location":"models/ViT/#jimm.models.vit.VisionTransformer.__call__","title":"<code>__call__(x)</code>","text":"<p>Forward pass of the Vision Transformer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, 'batch height width channels']</code> <p>Input tensor with shape [batch, height, width, channels]</p> required <p>Returns:</p> Type Description <code>Float[Array, 'batch num_classes']</code> <p>Float[Array, \"batch num_classes\"]: Output logits with shape [batch, num_classes]</p> Source code in <code>src/jimm/models/vit.py</code> <pre><code>def __call__(self, x: Float[Array, \"batch height width channels\"]) -&gt; Float[Array, \"batch num_classes\"]:\n    \"\"\"Forward pass of the Vision Transformer.\n\n    Args:\n        x (Float[Array, \"batch height width channels\"]): Input tensor with shape [batch, height, width, channels]\n\n    Returns:\n        Float[Array, \"batch num_classes\"]: Output logits with shape [batch, num_classes]\n    \"\"\"\n    patches = self.patch_embeddings(x)\n    batch_size = patches.shape[0]\n    patches = patches.reshape(batch_size, -1, patches.shape[-1])\n    cls_token = jnp.tile(self.cls_token.value, [batch_size, 1, 1])\n    x = jnp.concat([cls_token, patches], axis=1)\n    embeddings = x + self.position_embeddings.value\n    embeddings = self.dropout(embeddings)\n    x = self.encoder(embeddings)\n    x = self.final_norm(x)\n    x = x[:, 0]\n    return self.classifier(x)\n</code></pre>"},{"location":"models/ViT/#jimm.models.vit.VisionTransformer.__init__","title":"<code>__init__(num_classes=1000, in_channels=3, img_size=224, patch_size=16, num_layers=12, num_heads=12, mlp_dim=3072, hidden_size=768, dropout_rate=0.1, use_quick_gelu=False, dtype=jnp.float32, param_dtype=jnp.float32, rngs=nnx.Rngs(0), mesh=None)</code>","text":"<p>Initialize a Vision Transformer.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Number of output classes. Defaults to 1000.</p> <code>1000</code> <code>in_channels</code> <code>int</code> <p>Number of input channels. Defaults to 3.</p> <code>3</code> <code>img_size</code> <code>int</code> <p>Size of the input image (assumed square). Defaults to 224.</p> <code>224</code> <code>patch_size</code> <code>int</code> <p>Size of each patch (assumed square). Defaults to 16.</p> <code>16</code> <code>num_layers</code> <code>int</code> <p>Number of transformer layers. Defaults to 12.</p> <code>12</code> <code>num_heads</code> <code>int</code> <p>Number of attention heads. Defaults to 12.</p> <code>12</code> <code>mlp_dim</code> <code>int</code> <p>Size of the MLP dimension. Defaults to 3072.</p> <code>3072</code> <code>hidden_size</code> <code>int</code> <p>Size of the hidden dimension. Defaults to 768.</p> <code>768</code> <code>dropout_rate</code> <code>float</code> <p>Dropout rate. Defaults to 0.1.</p> <code>0.1</code> <code>use_quick_gelu</code> <code>bool</code> <p>Whether to use quickgelu instead of gelu. Defaults to False.</p> <code>False</code> <code>dtype</code> <code>DTypeLike</code> <p>Data type for computations. Defaults to jnp.float32.</p> <code>float32</code> <code>param_dtype</code> <code>DTypeLike</code> <p>Data type for parameters. Defaults to jnp.float32.</p> <code>float32</code> <code>rngs</code> <code>Rngs</code> <p>Random number generator keys. Defaults to nnx.Rngs(0).</p> <code>Rngs(0)</code> <code>mesh</code> <code>Mesh | None</code> <p>Optional JAX device mesh for parameter sharding. Defaults to None.</p> <code>None</code> Source code in <code>src/jimm/models/vit.py</code> <pre><code>def __init__(\n    self,\n    num_classes: int = 1000,\n    in_channels: int = 3,\n    img_size: int = 224,\n    patch_size: int = 16,\n    num_layers: int = 12,\n    num_heads: int = 12,\n    mlp_dim: int = 3072,\n    hidden_size: int = 768,\n    dropout_rate: float = 0.1,\n    use_quick_gelu: bool = False,\n    dtype: DTypeLike = jnp.float32,\n    param_dtype: DTypeLike = jnp.float32,\n    rngs: nnx.Rngs = nnx.Rngs(0),\n    mesh: Mesh | None = None,\n) -&gt; None:\n    \"\"\"Initialize a Vision Transformer.\n\n    Args:\n        num_classes (int): Number of output classes. Defaults to 1000.\n        in_channels (int): Number of input channels. Defaults to 3.\n        img_size (int): Size of the input image (assumed square). Defaults to 224.\n        patch_size (int): Size of each patch (assumed square). Defaults to 16.\n        num_layers (int): Number of transformer layers. Defaults to 12.\n        num_heads (int): Number of attention heads. Defaults to 12.\n        mlp_dim (int): Size of the MLP dimension. Defaults to 3072.\n        hidden_size (int): Size of the hidden dimension. Defaults to 768.\n        dropout_rate (float): Dropout rate. Defaults to 0.1.\n        use_quick_gelu (bool): Whether to use quickgelu instead of gelu. Defaults to False.\n        dtype (DTypeLike): Data type for computations. Defaults to jnp.float32.\n        param_dtype (DTypeLike): Data type for parameters. Defaults to jnp.float32.\n        rngs (nnx.Rngs): Random number generator keys. Defaults to nnx.Rngs(0).\n        mesh (Mesh|None): Optional JAX device mesh for parameter sharding. Defaults to None.\n    \"\"\"\n    n_patches: int = (img_size // patch_size) ** 2\n    self.patch_embeddings = nnx.Conv(\n        in_features=in_channels,\n        out_features=hidden_size,\n        kernel_size=(patch_size, patch_size),\n        strides=(patch_size, patch_size),\n        padding=\"VALID\",\n        use_bias=True,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        rngs=rngs,\n        kernel_init=sharded_init(nnx.initializers.xavier_uniform(), P(None, None, None, \"model\"), mesh),\n        bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n    )\n    _position_embeddings_initializer = sharded_init(nnx.initializers.truncated_normal(stddev=0.02), P(None, None, \"model\"), mesh)\n    pos_emb_value: Float[Array, \"one n_patches_plus_1 hidden_size_dim\"] = _position_embeddings_initializer(rngs.params(), (1, n_patches + 1, hidden_size))\n    self.position_embeddings = nnx.Param(pos_emb_value)\n\n    self.dropout = nnx.Dropout(dropout_rate, rngs=rngs)\n\n    _cls_token_initializer = sharded_init(nnx.initializers.zeros_init(), P(None, None, \"model\"), mesh)\n    cls_token_value: Float[Array, \"one one hidden_size_dim\"] = _cls_token_initializer(rngs.params(), (1, 1, hidden_size))\n    self.cls_token = nnx.Param(cls_token_value)\n\n    self.encoder = Transformer(\n        width=hidden_size,\n        mlp_dim=mlp_dim,\n        layers=num_layers,\n        num_heads=num_heads,\n        dropout_rate=dropout_rate,\n        use_quick_gelu=use_quick_gelu,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        rngs=rngs,\n        mesh=mesh,\n    )\n\n    self.final_norm = nnx.LayerNorm(\n        hidden_size,\n        epsilon=1e-12,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        rngs=rngs,\n        scale_init=sharded_init(nnx.initializers.ones_init(), P(\"model\"), mesh),\n        bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n    )\n    self.classifier = nnx.Linear(\n        hidden_size,\n        num_classes,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        rngs=rngs,\n        kernel_init=sharded_init(nnx.initializers.xavier_uniform(), P(None, \"model\"), mesh),\n        bias_init=sharded_init(nnx.initializers.zeros_init(), P(\"model\"), mesh),\n    )\n</code></pre>"},{"location":"models/ViT/#jimm.models.vit.VisionTransformer.from_pretrained","title":"<code>from_pretrained(model_name_or_path, use_pytorch=False, mesh=None, dtype=jnp.float32)</code>  <code>classmethod</code>","text":"<p>Load a pretrained Vision Transformer from a local path or HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_path</code> <code>str</code> <p>Path to local weights or HuggingFace model ID.</p> required <code>use_pytorch</code> <code>bool</code> <p>Whether to load from PyTorch weights. Defaults to False.</p> <code>False</code> <code>mesh</code> <code>Mesh | None</code> <p>Optional device mesh for parameter sharding. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>DTypeLike</code> <p>Data type for computations. Defaults to jnp.float32.</p> <code>float32</code> <p>Returns:</p> Name Type Description <code>VisionTransformer</code> <code>VisionTransformer</code> <p>Initialized Vision Transformer with pretrained weights</p> Source code in <code>src/jimm/models/vit.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, model_name_or_path: str, use_pytorch: bool = False, mesh: Mesh | None = None, dtype: DTypeLike = jnp.float32) -&gt; \"VisionTransformer\":\n    \"\"\"Load a pretrained Vision Transformer from a local path or HuggingFace Hub.\n\n    Args:\n        model_name_or_path (str): Path to local weights or HuggingFace model ID.\n        use_pytorch (bool): Whether to load from PyTorch weights. Defaults to False.\n        mesh (Mesh|None): Optional device mesh for parameter sharding. Defaults to None.\n        dtype (DTypeLike): Data type for computations. Defaults to jnp.float32.\n\n    Returns:\n        VisionTransformer: Initialized Vision Transformer with pretrained weights\n    \"\"\"\n    params_fstate, config_dict = load_params_and_config(model_name_or_path, use_pytorch)\n\n    config: dict[str, Any] | None = config_dict\n\n    hidden_size_val: int\n    num_classes_val: int\n    num_layers_val: int\n    num_heads_val: int\n    mlp_dim_val: int\n    patch_size_val: int\n    img_size_val: int\n    use_quick_gelu_val: bool = False\n\n    if config:\n        hidden_size_val = config[\"hidden_size\"]\n        num_classes_val = len(config[\"id2label\"]) if \"id2label\" in config else config.get(\"num_labels\", 1000)\n        num_layers_val = config[\"num_hidden_layers\"]\n        num_heads_val = config[\"num_attention_heads\"]\n        mlp_dim_val = config[\"intermediate_size\"]\n        patch_size_val = config[\"patch_size\"]\n        img_size_val = config[\"image_size\"]\n        if \"hidden_act\" in config and config[\"hidden_act\"] == \"quick_gelu\":\n            use_quick_gelu_val = True\n        elif \"hidden_act\" in config and config[\"hidden_act\"] != \"gelu\":\n            print(f\"Warning: Unexpected hidden_act '{config['hidden_act']}' in config, defaulting to standard GELU.\")\n\n    elif not use_pytorch and (os.path.exists(model_name_or_path) and os.path.isfile(model_name_or_path)):\n        hidden_size_val = params_fstate[\"vit.embeddings.cls_token\"].shape[-1]\n        num_classes_val = params_fstate[\"classifier.bias\"].shape[0]\n\n        max_layer_idx = -1\n        for k in params_fstate:\n            if k.startswith(\"vit.encoder.layer.\"):\n                max_layer_idx = max(max_layer_idx, int(k.split(\".\")[3]))\n        num_layers_val = max_layer_idx + 1\n\n        mlp_dim_val = params_fstate[\"vit.encoder.layer.0.intermediate.dense.weight\"].shape[0]\n\n        assumed_head_dim = 64\n        num_heads_val = hidden_size_val // assumed_head_dim\n\n        patch_kernel_shape = params_fstate[\"vit.embeddings.patch_embeddings.projection.weight\"].shape\n        patch_size_val = patch_kernel_shape[2]\n\n        num_patches_from_embeddings = params_fstate[\"vit.embeddings.position_embeddings\"].shape[1] - 1\n        img_size_dim = int(jnp.sqrt(num_patches_from_embeddings))\n        img_size_val = img_size_dim * patch_size_val\n    else:\n        raise ValueError(f\"Could not load or infer configuration for {model_name_or_path}\")\n\n    if not all(v is not None for v in [hidden_size_val, num_classes_val, num_layers_val, num_heads_val, mlp_dim_val, patch_size_val, img_size_val]):\n        raise ValueError(f\"One or more configuration parameters could not be determined for {model_name_or_path}\")\n\n    model = cls(\n        num_classes=num_classes_val,\n        img_size=img_size_val,\n        patch_size=patch_size_val,\n        num_layers=num_layers_val,\n        num_heads=num_heads_val,\n        mlp_dim=mlp_dim_val,\n        hidden_size=hidden_size_val,\n        use_quick_gelu=use_quick_gelu_val,\n        mesh=mesh,\n        dtype=dtype,\n        param_dtype=dtype,\n    )\n\n    flax_model_params_fstate = dict(nnx.to_flat_state(nnx.state(model, nnx.Param)))\n\n    def hf_param_name(name: str) -&gt; str:\n        return \"weight\" if name in [\"kernel\", \"scale\"] else name\n\n    hidden_size_per_head = hidden_size_val // num_heads_val\n\n    mapping_list = [\n        ((\"cls_token\",), (\"vit\", \"embeddings\", \"cls_token\")),\n        ((\"position_embeddings\",), (\"vit\", \"embeddings\", \"position_embeddings\")),\n        ((\"patch_embeddings\", \"kernel\"), (\"vit\", \"embeddings\", \"patch_embeddings\", \"projection\", \"weight\")),\n        ((\"patch_embeddings\", \"bias\"), (\"vit\", \"embeddings\", \"patch_embeddings\", \"projection\", \"bias\")),\n        ((\"classifier\", \"kernel\"), (\"classifier\", \"weight\")),\n        ((\"classifier\", \"bias\"), (\"classifier\", \"bias\")),\n        ((\"final_norm\", \"scale\"), (\"vit\", \"layernorm\", \"weight\")),\n        ((\"final_norm\", \"bias\"), (\"vit\", \"layernorm\", \"bias\")),\n    ]\n\n    for i in range(num_layers_val):\n        flax_base = (\"encoder\", \"blocks\", \"layers\", i)\n        hf_base = (\"vit\", \"encoder\", \"layer\", str(i))\n        mapping_list.extend(\n            [(flax_base + (\"attn\", y_type, p_name), hf_base + (\"attention\", \"attention\", y_type, hf_param_name(p_name))) for p_name in [\"kernel\", \"bias\"] for y_type in [\"key\", \"value\", \"query\"]]\n        )\n        mapping_list.extend([(flax_base + (\"attn\", \"out\", p_name), hf_base + (\"attention\", \"output\", \"dense\", hf_param_name(p_name))) for p_name in [\"kernel\", \"bias\"]])\n        mapping_list.extend(\n            [\n                (flax_base + (\"mlp\", \"layers\", y1_idx, p_name), hf_base + (y2_name, \"dense\", hf_param_name(p_name)))\n                for p_name in [\"kernel\", \"bias\"]\n                for y1_idx, y2_name in [(0, \"intermediate\"), (3, \"output\")]\n            ]\n        )\n        mapping_list.extend(\n            [\n                (flax_base + (norm_flax, p_name), hf_base + (norm_hf, hf_param_name(p_name)))\n                for p_name in [\"scale\", \"bias\"]\n                for norm_flax, norm_hf in [(\"norm1\", \"layernorm_before\"), (\"norm2\", \"layernorm_after\")]\n            ]\n        )\n    params_name_mapping = dict(mapping_list)\n    nonvisited = set(flax_model_params_fstate.keys())\n    used_hf_keys: Set[str] = set()\n\n    for flax_dst_key_tuple, hf_src_key_tuple in params_name_mapping.items():\n        assert flax_dst_key_tuple in flax_model_params_fstate, flax_dst_key_tuple\n        hf_src_key_as_string = \".\".join(hf_src_key_tuple)\n        used_hf_keys.add(hf_src_key_as_string)\n        assert hf_src_key_as_string in params_fstate, f\"HF key '{hf_src_key_as_string}' (from Flax key {flax_dst_key_tuple}) not found in loaded weights.\"\n        nonvisited.remove(flax_dst_key_tuple)\n        src_value: Array = params_fstate[hf_src_key_as_string]\n\n        dst_value_obj = flax_model_params_fstate[flax_dst_key_tuple]\n        original_param_sharding = dst_value_obj.value.sharding\n\n        if flax_dst_key_tuple == (\"patch_embeddings\", \"kernel\"):\n            src_value = jnp.transpose(src_value, (2, 3, 1, 0))\n        elif hf_src_key_tuple[-1] == \"weight\" and hf_src_key_tuple[-2] in (\"key\", \"value\", \"query\"):\n            src_value = jnp.transpose(src_value, (1, 0))\n            src_value = src_value.reshape((hidden_size_val, num_heads_val, hidden_size_per_head))\n        elif hf_src_key_tuple[-1] == \"bias\" and hf_src_key_tuple[-2] in (\"key\", \"value\", \"query\"):\n            src_value = src_value.reshape((num_heads_val, hidden_size_per_head))\n        elif hf_src_key_tuple[-4:] == (\"attention\", \"output\", \"dense\", \"weight\"):\n            src_value = jnp.transpose(src_value, (1, 0))\n            src_value = src_value.reshape((num_heads_val, hidden_size_per_head, hidden_size_val))\n        elif hf_src_key_tuple[-1] == \"weight\" and src_value.ndim == 2:\n            src_value = jnp.transpose(src_value, (1, 0))\n\n        assert src_value.shape == dst_value_obj.value.shape, f\"Shape mismatch for {flax_dst_key_tuple} (Flax) vs {hf_src_key_as_string} (HF): {dst_value_obj.value.shape} != {src_value.shape}\"\n\n        sharded_new_value: Array = jax.device_put(src_value, original_param_sharding)\n        dst_value_obj.value = sharded_new_value\n\n        assert jnp.allclose(dst_value_obj.value.mean(), src_value.mean()), (dst_value_obj.value.mean(), src_value.mean())\n\n    assert len(nonvisited) == 0, f\"Some Flax model parameters were not visited: {nonvisited}\"\n\n    leftover_hf_keys = set(params_fstate.keys()) - used_hf_keys\n    known_unused_hf_buffer_keys = {\n        \"text_model.embeddings.position_ids\",\n        \"vision_model.embeddings.position_ids\",\n    }\n    unexpected_leftover_hf_keys = leftover_hf_keys - known_unused_hf_buffer_keys\n\n    assert len(unexpected_leftover_hf_keys) == 0, f\"Some unexpected HuggingFace checkpoint parameters were not used: {sorted(list(unexpected_leftover_hf_keys))}\"\n    nnx.update(model, nnx.from_flat_state(flax_model_params_fstate))\n\n    del flax_model_params_fstate\n    del params_fstate\n    return model\n</code></pre>"}]}